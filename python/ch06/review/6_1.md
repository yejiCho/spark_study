# 스파크 스트리밍

<!-- ANCHOR 스파크스트리밍 정의 -->
```
주어진 데이터를 읽고 처리하는 것 뿐만 아니라 시간의 흐름에 따라 꾸준히 변화하는
데이터를다루기 위한 것.

대용량 데이터를 다루는 경우에는 데이터베이스를 활용한 처리가 불가능한 경우가 있는데,

이런 성격의 데이터를 다루기 위한 스파크의 서브 모듈로서 실시간으로 변화하는
데이터를(배치 처리보다) 짧은 주기에 맞춰 안정적으로 처리하는 데 필요한 기능 제공
```

## 스트리밍컨텍스트

<!-- ANCHOR streamingcontext -->
```python

스트리밍 모듈을 사용하기 위해서는 스트리밍컨텍스트(StreamingContext)인스턴스를 생성
어떤 주기로 배치 처리를 수행할지에 대한 정보(bathDuration)를 함께 제공

from pyspark import SparkContext, SparkConf, storagelevel
from pyspark.streaming.context import StreamingContext

conf = SparkConf()
conf.set("spark.drvier.host",'127.0.0.1')
sc = SparkContext(master="local",appName="StreamingSample",conf=conf)
ssc = StreamingContext(sc,3)

```

- 특징

1. 명시적인 시작(start),종료(stop),대기(awaitTermination) 메서드를 가지고 있다.

> 스파크컨텍스트와 스파크세션과는 달리 명시적으로 메서드를 호출해서 시작,종료를 시켜줘야한다.

2. 단 한번 시작되고 종료합니다. 즉 한번 종료한 스트리밍컨텍스트를 다시 재시작할 수 없습니다.

3. 한번 시작되고 나면 더 이상 새로운 연산을 정의하거나 추가할 수 없습니다.

4. JVM당 하나의 스트리밍 컨텍스트만 동시에 활성화 가능 

5. stop() 메서드를 호출하면 연관된 SparkContext도 함께 중지.

> 스트리밍컨텍스트만 종료하고 싶을 경우 stop() 메서드의 stopSparkContext 매개변수 값을 false로 지정

6. 한번에 하나의 스트리밍컨텍스트만 동작한다는 가정하에 하나의 스파크컨텍스트로부터 여러 개의 스트리밍 컨텍스트를 생성

## Dstream(Discretized Streams)

<!-- ANCHOR Dstream -->
```
스트리밍 컨텍스트를 생성한 뒤 데이터를 읽고 스파크에서 사용할 데이터 모델 인스턴스 생성

스파크컨텍스트를 이용해 RDD를 만들고 스파크세션을 이용해 데이터셋 또는 데이터프레임 만드는 단계

스파크스트리밍에서는 Dstream 데이터 모델을 사용하는데, 고정되지 않고 끊임없이 생성되는 연속된 데이터를
나타내기 위한 일종의 추상모델

```

연속된 데이터를 다루는 방법

- 시간 간격 사이에 새로 생성된 데이터를 모아서 한 번에 처리하는 방식

Dstream의 경우에는 같은 방식으로 데이터스트림을 처리해서 일정 시간마다 데이터를 모아서 RDD를만드는데
이러한 RDD로 구성된 시퀀스가 바로 Dstream이라고 할 수 있습니다.

> 앞에서 스트리밍컨텍스트를 생성할 때 배치 간격에 대한 정보(batchDuration)가 필요하다고 했는데
이 매개변수가 연속된 데이터스트림으로 데이터를 읽어들일 시간 간격을 의미


## 데이터 읽기

스트림컨텍스트가 지원하는 데이터소스

- 기본 데이터소스

ex) 소켓,파일,RDD 큐

- 외부라이브러리가 필요한 어드밴스드 데이터소스

ex) 카프카(kafka),플럼(flume),키니시스(kinesis),트위터(twitter)

- 스파크가 제공하는 Receiver 추상클래스 상속받아 자체적으로 정의한 데이터소스


## 기본데이터소스

- 소켓(socket)
<!-- ANCHOR 소켓(socket) -->
```
TCP 소켓을 이용해 데이터를 수신하는 경우 서버의 IP와 포트 번호를 지정해 
스파크 스트리밍의 데이터소스로 사용가능

socketTextStream() : 소켓을 통해 문자열 데이터를 수신

문자열이 아닌 경우
socketStream() 메서드에 데이터 타입 변환을 위한 converter()함수를 지정해 사용

```

```python

from pyspark import SparkConf, SparkContext, storagelevel
from pyspark.streaming.context import StreamingContext
import queue

conf = SparkConf()
conf.set("spark.driver.host","127.0.0.1")

sc = SparkContext(master="local",appName="SocketSample",conf=conf)
ssc = StreamingContext(sc,3)

'''
NOTE
Dtream 생성부분
localhost : 데이터소스가 될 서버의 주소
9000 : 포트번호

실제테스트를 위해서는 IP와 포트를 사용하는 TCP 서버가 필요하므로
Netcat 서버를 이용
nc -lk 9000
'''
ds = ssc.socketTextStream("localhost",9000)

```

- 파일

```python
데이터소스로 파일을 사용할 경우 fileStream() 메서드 사용
데이터 형식이 문자열일 경우에는 textFileStream() 메서드 사용

ds = ssc.textFileStream(<path>)

```

- RDD 큐 (Queue of RDD)

```python
# RDD로 구선된 Queue를 이용해 Dstream 생성가능
# 테스트케이스 만들기에 가장 적합한 방법
from pyspark import SparkConf, SparkContext
from pyspark.streaming.context import StreamingContext

conf = SparkConf()
sc = SparkContext(master="local[*]",appName="Streaming",conf=conf)
ssc = StreamingContext(sc,1)

rdd1 = ssc.sparkContext.parallelize(["a", "b", "c", "c", "c"])
q1 = [rdd1]

ds1 = ssc.queueStream(q1, True)

```

## 데이터 다루기 (기본연산)

<!-- ANCHOR 기본연산 -->

- print()

Dstream에 포함된 각 RDD의 내용을 콘솔에 출력

```python

rdd1 = ssc.SparkContext.parallelize(["a","b","c","c","c"])

q1 = [rdd1]


ds1 = ssc.queueStream(q1,True)

ds1.pprint()

```

- map(func)

```python

ds1.map(lambda v:(v,1)).pprint()

```

- flatmap(func)

```python
```
NOTE flatmap
입력과 출력이 1:1로 매핑되는 map()연산과는 달리
하나의 입력이 0~N개의 출력으로 반환된다는 차이점
```

rdd2 = ssc.sparkContext.parallelize(["1,2,3,4,5"])

q2 = [rdd2]

ds2 = ssc.queueStream(q2,True)

ds2.flatmap(lambda v:v.split(",")).pprint()

```

- count(),countByValue()

```python
# Dstream에 포함된 요소의 개수
ds1.count().pprint()
# Dstream에 포함된 요소(요소,해당요소의 개수)
ds1.countByValue().pprint()

```

## 고급연산

- transform(func)

```
Dstream 내부의 RDD에 func함수를 적용하고 그 결과로 새로운
Dstream을 반환

매개변수로 전달되는 func함수는 입력과 출력이 모두 RDD타입으로,
Dstream내부의 RDD에 접근해 원하는 RDD연산을 직접 수행할 수 있다.


```

- updateStateByKey()

```
키와 값 형태로 구성된 데이터의 경우 가장 자주 사용되는 데이터 처리 방식중
하나가 키를 기준으로 전체 데이터 값을 요약하는 것

상태유지(stateful)방식의 집계를 수행할 수 없기 때문에
전체 데이터를 집계하기 위해서는 별도의 집계기능을 구현

updateStateByKey()
같은 키를 가진 데이터를 모아서 사용자가 지정한 함수의 인자로 전달해 주면서
이전 배치까지의 최종값도 함께 전달해주는 메소드

새로 생성된 데이터와 이전 배치의 최종 상태값을 함께 전달해주기 때문에
각 키별 최신값, 즉 상태(state)유지, 갱신 가능

```

```python

#  REVIEW updateStateByKey()

1. 3초 주기로 데이터를 처리하는 스트리밍컨텍스트 ssc 생성
2. 3개의 RDD생성 
3. queueStream()을 이용해 Dstream() 생성

# t1,t2,t3,RDD를 원소로 지정하고 배치가 수행될 때마다 한개씩 RDD를 읽어들이도록함

queuestream()메소드

True일 경우 매번 배치가 수행될때마다 RDD를 한개씩만 읽어들이기
False일 경우 모든 RDD가 한 번의 배치에 모두 포함

# checkpoint 설정
ssc.checkpoint('.')

checkpoint() 메소드는 현재의 작업 상태를 HDFS와 같은 영속성을 가진 저장소에 저장해 놓는 메소드

작업 도중 상태값이 유실되는 것을 방지하기 위해 중간 임시저장

상태 갱신을 위한 함수

첫번째 인자 : 현재 Dstream에 있는 값들로 같은 키를 가진 값들이 Seq 타입으로 전달
두전째 인자 : 현재 해당키의 상태 (예제. 단어 수를 나타내는 Long값)

마지막으로 반환되는 값 또한 Option타입으로 지정하는데, 이는 반환 값이 없을 경우
해당 키를 최종 결과에서 제외시킴

즉 updateFunc의 수행 결과가 None일 경우 해당 키와 값 쌍은 제외

```

## 윈도우 연산

```python
윈도우 연산은 마지막 배치가 수행됐을 때 읽어온 데이터 뿐만 아니라
그 이전에 수행된 배치의 입력 데이터까지 한꺼번에 처리할 수 있도록 지원하는 연산

- 얼마만큼의 간격으로 윈도우 연산을 수행할 것인지
- 한번 수행할 때 얼마만큼의 과거 배치 수행 결과를 가져올 것인지

ex) 스트리밍컨텍스트 생성시 배치주기를 3초로 했다면, 슬라이딩 간격은 3초의 배수로 설정

sc = ssc.sparkContext
input = [sc.parallelize([i]) for i in range(1,100)]
ds7 = ssc.queueStream(input)

```